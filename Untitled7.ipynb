{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMaYQZDJSK8NeBdbZmp6Jq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/i-ganza007/AlaMovie/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "W5p1YitnldqJ"
      },
      "outputs": [],
      "source": [
        "import librosa as lb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, log_loss , precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_files(audio_files):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for file_path, user, phrase in audio_files:\n",
        "        y, sr = lb.load(file_path)\n",
        "        yt, _ = lb.effects.trim(y)\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        lb.display.waveshow(yt, sr=sr)\n",
        "        plt.title(f'Waveform: {user} - {phrase}')\n",
        "        plt.savefig(f\"waveform_{user}_{phrase.replace(' ', '_')}.png\")\n",
        "        plt.close()\n",
        "        print(f\"Saved waveform for {user} - {phrase}: Duration {lb.get_duration(y=yt, sr=sr):.2f}s, clear speech peaks\")\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        D = lb.amplitude_to_db(np.abs(lb.stft(yt)), ref=np.max)\n",
        "        lb.display.specshow(D, sr=sr, x_axis='time', y_axis='hz')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Spectrogram: {user} - {phrase}')\n",
        "        plt.savefig(f\"Spectogram{user}_{phrase.replace(' ', '_')}.png\")\n",
        "        plt.close()\n",
        "        print(f\"Saved spectrogram for {user} - {phrase}: Speech frequencies 0-5kHz\")\n",
        "        y_pitch = lb.effects.pitch_shift(yt, sr=sr, n_steps=4)\n",
        "        y_stretch = lb.effects.time_stretch(yt, rate=0.8)\n",
        "        for audio, aug_type in [(yt, 'original'), (y_pitch, 'pitch'), (y_stretch, 'stretch')]:\n",
        "            mfcc = lb.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
        "            features.append({\n",
        "                'file': f\"{file_path.split('/')[-1]}_{aug_type}\",\n",
        "                'user': user,\n",
        "                'phrase': phrase,\n",
        "                'mfcc_mean': np.mean(mfcc, axis=1).tolist(),\n",
        "                'rolloff_mean': float(np.mean(lb.feature.spectral_rolloff(y=audio, sr=sr))),\n",
        "                'energy_mean': float(np.mean(lb.feature.rms(y=audio)))\n",
        "            })\n",
        "            labels.append(f\"{user}_{phrase.lower()}\")\n",
        "    df_augmented = pd.DataFrame(features)\n",
        "    df_augmented.to_csv('audio_features_augmented.csv', index=False)\n",
        "    return df_augmented, np.array(labels)"
      ],
      "metadata": {
        "id": "DUDM8Su4zax4"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_combine_features(csv_path='audio_features.csv'):\n",
        "    # Load original features\n",
        "    df_original = pd.read_csv(csv_path)\n",
        "    # Map filenames to user-phrase classes\n",
        "    df_original['phrase'] = df_original['file'].apply(\n",
        "        lambda x: 'confirm transaction' if 'confirm' in x.lower() else 'yes approve'\n",
        "    )\n",
        "    df_original['user'] = df_original['file'].apply(\n",
        "        lambda x: 'Eddy' if 'eddy' in x.lower() else 'Lievin'\n",
        "    )\n",
        "    df_original['label'] = df_original.apply(lambda x: f\"{x['user']}_{x['phrase'].lower()}\", axis=1)\n",
        "    df_original['mfcc_mean'] = df_original['mfcc_mean'].apply(ast.literal_eval)\n",
        "\n",
        "    # Process audio files for augmented features\n",
        "    audio_files = [\n",
        "        ('/content/confirm_eddy.mp3', 'Eddy', 'confirm transaction'),\n",
        "        ('/content/confirm_lievin.mp3', 'Lievin', 'confirm transaction'),\n",
        "        ('/content/yes_approve_eddy.mp3', 'Eddy', 'yes approve'),\n",
        "        ('/content/yes_approve_lievin.mp3', 'Lievin', 'yes approve')\n",
        "    ]\n",
        "    df_augmented, augmented_labels = process_audio_files(audio_files)\n",
        "\n",
        "    # Combine features\n",
        "    df_combined = pd.concat([df_original, df_augmented], ignore_index=True)\n",
        "    labels = np.concatenate([df_original['label'].values, augmented_labels])\n",
        "\n",
        "    # Extract feature vectors\n",
        "    X = np.array([\n",
        "        row['mfcc_mean'] + [row['rolloff_mean'], row['energy_mean']]\n",
        "        for _, row in df_combined.iterrows()\n",
        "    ])\n",
        "\n",
        "    return X, labels, df_combined"
      ],
      "metadata": {
        "id": "IWfBiqwayVIS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rf_model(features, labels):\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Random Forest\n",
        "    model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    loss = log_loss(y_test, y_prob, labels=le.classes_)\n",
        "\n",
        "    print(f\"\\nVoiceprint RF Model - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, Log Loss: {loss:.4f}\")\n",
        "\n",
        "    return model, le"
      ],
      "metadata": {
        "id": "nlBoAuKny0GK"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_transaction(audio_path, model, label_encoder):\n",
        "    print(\"\\n=== Voiceprint Verification Simulation ===\")\n",
        "\n",
        "    # Load and preprocess audio\n",
        "    y, sr = lb.load(audio_path)\n",
        "    yt, _ = lb.effects.trim(y)\n",
        "    mfcc = lb.feature.mfcc(y=yt, sr=sr, n_mfcc=20)\n",
        "    features = np.concatenate([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        [np.mean(lb.feature.spectral_rolloff(y=yt, sr=sr))],\n",
        "        [np.mean(lb.feature.rms(y=yt))]\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Predict\n",
        "    pred = model.predict(features)\n",
        "    pred_label = label_encoder.inverse_transform(pred)[0]\n",
        "    pred_user, pred_phrase = pred_label.split('_', 1)\n",
        "\n",
        "    # Verify authorized user (Eddy or Lievin) and valid sentence\n",
        "    if pred_user in ['Eddy', 'Lievin'] and pred_phrase in ['yes approve', 'confirm transaction']:\n",
        "        print(f\"Voice verification passed. Predicted: {pred_user} saying '{pred_phrase}'\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Access Denied: Predicted {pred_user} saying '{pred_phrase}', expected Eddy or Lievin saying 'yes approve' or 'confirm transaction'\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "InxVUx3Z19Fb"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_unauthorized_attempt(model, label_encoder):\n",
        "    print(\"\\n=== Unauthorized Voice Attempt Simulation ===\")\n",
        "\n",
        "    # Simulate random features (representing an unauthorized user)\n",
        "    features = np.concatenate([\n",
        "        np.random.randn(20) * 10,  # Random MFCC means\n",
        "        [np.random.uniform(1000, 5000)],  # Random rolloff\n",
        "        [np.random.uniform(0.01, 0.1)]  # Random energy\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Predict\n",
        "    pred = model.predict(features)\n",
        "    pred_label = label_encoder.inverse_transform(pred)[0]\n",
        "    pred_user, pred_phrase = pred_label.split('_', 1)\n",
        "    print(f\"Access Denied: Predicted {pred_user} saying '{pred_phrase}' (unauthorized user)\")"
      ],
      "metadata": {
        "id": "jTADVB9b98aF"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load and combine features\n",
        "    X, labels, df_combined = load_and_combine_features('/content/audio_features(1).csv')\n",
        "\n",
        "    # Train model\n",
        "    model, label_encoder = train_rf_model(X, labels)\n",
        "\n",
        "    # Simulate authorized transaction (example: Eddy saying 'confirm transaction')\n",
        "    print(\"\\n--- Running Authorized Transaction Simulation ---\") # Added header\n",
        "    simulate_transaction(\n",
        "        '/content/confirm_eddy.mp3',\n",
        "        model=model,\n",
        "        label_encoder=label_encoder\n",
        "    )\n",
        "\n",
        "    # Simulate unauthorized attempt\n",
        "    print(\"\\n--- Running Unauthorized Attempt Simulation ---\") # Added header\n",
        "    simulate_unauthorized_attempt(model, label_encoder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdbgMK0O-PK0",
        "outputId": "f8a81afb-4e23-487a-8667-8ab3963efc6a"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved waveform for Eddy - confirm transaction: Duration 2.67s, clear speech peaks\n",
            "Saved spectrogram for Eddy - confirm transaction: Speech frequencies 0-5kHz\n",
            "Saved waveform for Lievin - confirm transaction: Duration 2.04s, clear speech peaks\n",
            "Saved spectrogram for Lievin - confirm transaction: Speech frequencies 0-5kHz\n",
            "Saved waveform for Eddy - yes approve: Duration 2.88s, clear speech peaks\n",
            "Saved spectrogram for Eddy - yes approve: Speech frequencies 0-5kHz\n",
            "Saved waveform for Lievin - yes approve: Duration 2.85s, clear speech peaks\n",
            "Saved spectrogram for Lievin - yes approve: Speech frequencies 0-5kHz\n",
            "\n",
            "Voiceprint RF Model - Accuracy: 1.0000, F1-Score: 1.0000, Log Loss: 0.0000\n",
            "\n",
            "--- Running Authorized Transaction Simulation ---\n",
            "\n",
            "=== Voiceprint Verification Simulation ===\n",
            "Voice verification passed. Predicted: Eddy saying 'confirm transaction'\n",
            "\n",
            "--- Running Unauthorized Attempt Simulation ---\n",
            "\n",
            "=== Unauthorized Voice Attempt Simulation ===\n",
            "Access Denied: Predicted Eddy saying 'yes approve' (unauthorized user)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd301b6"
      },
      "source": [
        "# Task\n",
        "Modify the provided Python code to take a specific voice sample file as input, process it using the trained model, and based on the prediction, print either a \"Verification successful\" message or an \"Access denied\" message."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b338c422"
      },
      "source": [
        "## Create a new function to process a voice sample and predict\n",
        "\n",
        "### Subtask:\n",
        "Define a function `process_and_predict_voice` that takes an audio file path, the trained model, and the label encoder as input. This function will load and preprocess the audio, predict the user and phrase using the provided model, and return the predicted user and phrase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520ddb31"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `process_and_predict_voice` function as described in the instructions to preprocess audio, extract features, and predict the user and phrase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55a1e42b"
      },
      "source": [
        "def process_and_predict_voice(audio_path, model, label_encoder):\n",
        "    \"\"\"Loads and preprocesses an audio file, predicts the user and phrase using the provided model.\n",
        "\n",
        "    Args:\n",
        "        audio_path (str): Path to the audio file.\n",
        "        model: Trained machine learning model.\n",
        "        label_encoder: Label encoder used to transform labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted user (str) and predicted phrase (str).\n",
        "    \"\"\"\n",
        "    # Load and preprocess audio\n",
        "    y, sr = lb.load(audio_path)\n",
        "    yt, _ = lb.effects.trim(y)\n",
        "\n",
        "    # Extract features\n",
        "    mfcc = lb.feature.mfcc(y=yt, sr=sr, n_mfcc=20)\n",
        "    rolloff_mean = float(np.mean(lb.feature.spectral_rolloff(y=yt, sr=sr)))\n",
        "    energy_mean = float(np.mean(lb.feature.rms(y=yt)))\n",
        "\n",
        "    # Combine features\n",
        "    features = np.concatenate([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        [rolloff_mean],\n",
        "        [energy_mean]\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Predict\n",
        "    pred = model.predict(features)\n",
        "    pred_label = label_encoder.inverse_transform(pred)[0]\n",
        "    pred_user, pred_phrase = pred_label.split('_', 1)\n",
        "\n",
        "    return pred_user, pred_phrase"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "104e11d6"
      },
      "source": [
        "## Modify the main execution flow\n",
        "\n",
        "### Subtask:\n",
        "Modify the `main` function to accept an audio file path as input, and then use this path to call the `process_and_predict_voice` function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7efd106e"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main function to accept an input audio path, remove the old simulation calls, and call the new processing and prediction function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe028464"
      },
      "source": [
        "def main(input_audio_path):\n",
        "    # Load and combine features\n",
        "    X, labels, df_combined = load_and_combine_features('/content/audio_features(1).csv')\n",
        "\n",
        "    # Train model\n",
        "    model, label_encoder = train_rf_model(X, labels)\n",
        "\n",
        "    # Process the input audio file\n",
        "    print(f\"\\n--- Processing input audio file: {input_audio_path} ---\")\n",
        "    predicted_user, predicted_phrase = process_and_predict_voice(\n",
        "        input_audio_path,\n",
        "        model=model,\n",
        "        label_encoder=label_encoder\n",
        "    )\n",
        "\n",
        "    # The next subtask will handle printing the verification result\n",
        "    # based on predicted_user and predicted_phrase.\n",
        "    # For now, just print the prediction\n",
        "    print(f\"Predicted: {predicted_user} saying '{predicted_phrase}'\")\n",
        "\n",
        "\n",
        "# Example usage (will be removed or modified in the next step)\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Replace with the actual path to the audio file you want to test\n",
        "#     test_audio_path = '/content/confirm_eddy.mp3'\n",
        "#     main(test_audio_path)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7a95848"
      },
      "source": [
        "## Implement conditional logic\n",
        "\n",
        "### Subtask:\n",
        "Implement conditional logic in the `main` function to check if the predicted user and phrase from the input audio match an authorized user ('Eddy' or 'Lievin') and an authorized phrase ('yes approve' or 'confirm transaction').\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e78291b"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the conditional logic in the `main` function to check if the predicted user and phrase are authorized and print the corresponding verification message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7aea3da",
        "outputId": "47a35908-b3e0-4331-b403-73c16bfab351"
      },
      "source": [
        "def main(input_audio_path):\n",
        "    # Load and combine features\n",
        "    X, labels, df_combined = load_and_combine_features('/content/audio_features(1).csv')\n",
        "\n",
        "    # Train model\n",
        "    model, label_encoder = train_rf_model(X, labels)\n",
        "\n",
        "    # Process the input audio file\n",
        "    print(f\"\\n--- Processing input audio file: {input_audio_path} ---\")\n",
        "    predicted_user, predicted_phrase = process_and_predict_voice(\n",
        "        input_audio_path,\n",
        "        model=model,\n",
        "        label_encoder=label_encoder\n",
        "    )\n",
        "\n",
        "    # Implement conditional logic for verification\n",
        "    print(\"\\n=== Voiceprint Verification Result ===\")\n",
        "    if predicted_user in ['Eddy', 'Lievin'] and predicted_phrase in ['yes approve', 'confirm transaction']:\n",
        "        print(f\"Verification successful: Predicted {predicted_user} saying '{predicted_phrase}'.\")\n",
        "    else:\n",
        "        print(f\"Access denied: Predicted {predicted_user} saying '{predicted_phrase}'.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with the actual path to the audio file you want to test\n",
        "    test_audio_path = '/content/test_audio (online-audio-converter.com).mp3' # Example authorized voice\n",
        "    # test_audio_path = '/content/yes_approve_lievin.mp3' # Example authorized voice\n",
        "    # test_audio_path = '/content/unauthorized_voice.mp3' # Example unauthorized voice (if available)\n",
        "    main(test_audio_path)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved waveform for Eddy - confirm transaction: Duration 2.67s, clear speech peaks\n",
            "Saved spectrogram for Eddy - confirm transaction: Speech frequencies 0-5kHz\n",
            "Saved waveform for Lievin - confirm transaction: Duration 2.04s, clear speech peaks\n",
            "Saved spectrogram for Lievin - confirm transaction: Speech frequencies 0-5kHz\n",
            "Saved waveform for Eddy - yes approve: Duration 2.88s, clear speech peaks\n",
            "Saved spectrogram for Eddy - yes approve: Speech frequencies 0-5kHz\n",
            "Saved waveform for Lievin - yes approve: Duration 2.85s, clear speech peaks\n",
            "Saved spectrogram for Lievin - yes approve: Speech frequencies 0-5kHz\n",
            "\n",
            "Voiceprint RF Model - Accuracy: 1.0000, F1-Score: 1.0000, Log Loss: 0.0000\n",
            "\n",
            "--- Processing input audio file: /content/test_audio (online-audio-converter.com).mp3 ---\n",
            "\n",
            "=== Voiceprint Verification Result ===\n",
            "Verification successful: Predicted Eddy saying 'yes approve'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82da091a"
      },
      "source": [
        "## Execute success or failure scenario\n",
        "\n",
        "### Subtask:\n",
        "Based on the conditional logic implemented in the previous step, ensure that the appropriate success or failure message is printed to the console.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e8dfe6d"
      },
      "source": [
        "## Refine output\n",
        "\n",
        "### Subtask:\n",
        "Refine the output messages to be clear and informative based on the verification result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b33b483"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The code successfully defined a function `process_and_predict_voice` to load, preprocess, extract features, and predict the user and phrase from an audio file using a trained model.\n",
        "*   The `main` function was modified to accept an input audio file path and utilize the `process_and_predict_voice` function to obtain predictions.\n",
        "*   Conditional logic was successfully implemented in the `main` function to check if the predicted user ('Eddy' or 'Lievin') and predicted phrase ('yes approve' or 'confirm transaction') match the authorized criteria.\n",
        "*   Based on the prediction and the authorized criteria, the code now correctly prints either \"Verification successful\" or \"Access denied\", including the predicted user and phrase in the output.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current implementation assumes a fixed set of authorized users and phrases. A potential next step is to externalize this configuration (e.g., into a list or database) to make it easier to manage authorized users and phrases without modifying the code.\n",
        "*   Consider adding confidence scores or thresholds to the model's prediction to provide a more robust verification process, potentially allowing for fuzzy matching or handling slight variations in speech.\n"
      ]
    }
  ]
}