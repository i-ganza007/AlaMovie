{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPprCDwo389DwUWvan/R6De",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/i-ganza007/AlaMovie/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "W5p1YitnldqJ"
      },
      "outputs": [],
      "source": [
        "import librosa as lb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, log_loss , precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_files(audio_files):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for file_path, user, phrase in audio_files:\n",
        "        y, sr = lb.load(file_path)\n",
        "        yt, _ = lb.effects.trim(y)\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        lb.display.waveshow(yt, sr=sr)\n",
        "        plt.title(f'Waveform: {user} - {phrase}')\n",
        "        plt.savefig(f\"waveform_{user}_{phrase.replace(' ', '_')}.png\")\n",
        "        plt.close()\n",
        "        print(f\"Saved waveform for {user} - {phrase}: Duration {lb.get_duration(y=yt, sr=sr):.2f}s, clear speech peaks\")\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        D = lb.amplitude_to_db(np.abs(lb.stft(yt)), ref=np.max)\n",
        "        lb.display.specshow(D, sr=sr, x_axis='time', y_axis='hz')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Spectrogram: {user} - {phrase}')\n",
        "        plt.savefig(f\"Spectogram{user}_{phrase.replace(' ', '_')}.png\")\n",
        "        plt.close()\n",
        "        print(f\"Saved spectrogram for {user} - {phrase}: Speech frequencies 0-5kHz\")\n",
        "        y_pitch = lb.effects.pitch_shift(yt, sr=sr, n_steps=4)\n",
        "        y_stretch = lb.effects.time_stretch(yt, rate=0.8)\n",
        "        for audio, aug_type in [(yt, 'original'), (y_pitch, 'pitch'), (y_stretch, 'stretch')]:\n",
        "            mfcc = lb.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
        "            features.append({\n",
        "                'file': f\"{file_path.split('/')[-1]}_{aug_type}\",\n",
        "                'user': user,\n",
        "                'phrase': phrase,\n",
        "                'mfcc_mean': np.mean(mfcc, axis=1).tolist(),\n",
        "                'rolloff_mean': float(np.mean(lb.feature.spectral_rolloff(y=audio, sr=sr))),\n",
        "                'energy_mean': float(np.mean(lb.feature.rms(y=audio)))\n",
        "            })\n",
        "            labels.append(f\"{user}_{phrase.lower()}\")\n",
        "    df_augmented = pd.DataFrame(features)\n",
        "    df_augmented.to_csv('audio_features_augmented.csv', index=False)\n",
        "    return df_augmented, np.array(labels)"
      ],
      "metadata": {
        "id": "DUDM8Su4zax4"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_combine_features(csv_path='audio_features.csv'):\n",
        "    # Load original features\n",
        "    df_original = pd.read_csv(csv_path)\n",
        "    # Map filenames to user-phrase classes\n",
        "    df_original['phrase'] = df_original['file'].apply(\n",
        "        lambda x: 'confirm transaction' if 'confirm' in x.lower() else 'yes approve'\n",
        "    )\n",
        "    df_original['user'] = df_original['file'].apply(\n",
        "        lambda x: 'Eddy' if 'eddy' in x.lower() else 'Lievin'\n",
        "    )\n",
        "    df_original['label'] = df_original.apply(lambda x: f\"{x['user']}_{x['phrase'].lower()}\", axis=1)\n",
        "    df_original['mfcc_mean'] = df_original['mfcc_mean'].apply(ast.literal_eval)\n",
        "\n",
        "    # Process audio files for augmented features\n",
        "    audio_files = [\n",
        "        ('/content/confirm_eddy.mp3', 'Eddy', 'confirm transaction'),\n",
        "        ('/content/confirm_lievin.mp3', 'Lievin', 'confirm transaction'),\n",
        "        ('/content/yes_approve_eddy.mp3', 'Eddy', 'yes approve'),\n",
        "        ('/content/yes_approve_lievin.mp3', 'Lievin', 'yes approve')\n",
        "    ]\n",
        "    df_augmented, augmented_labels = process_audio_files(audio_files)\n",
        "\n",
        "    # Combine features\n",
        "    df_combined = pd.concat([df_original, df_augmented], ignore_index=True)\n",
        "    labels = np.concatenate([df_original['label'].values, augmented_labels])\n",
        "\n",
        "    # Extract feature vectors\n",
        "    X = np.array([\n",
        "        row['mfcc_mean'] + [row['rolloff_mean'], row['energy_mean']]\n",
        "        for _, row in df_combined.iterrows()\n",
        "    ])\n",
        "\n",
        "    return X, labels, df_combined"
      ],
      "metadata": {
        "id": "IWfBiqwayVIS"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rf_model(features, labels):\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Random Forest\n",
        "    model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    loss = log_loss(y_test, y_prob, labels=le.classes_)\n",
        "\n",
        "    print(f\"\\nVoiceprint RF Model - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, Log Loss: {loss:.4f}\")\n",
        "\n",
        "    return model, le"
      ],
      "metadata": {
        "id": "nlBoAuKny0GK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_transaction(audio_path, model, label_encoder):\n",
        "    print(\"\\n=== Voiceprint Verification Simulation ===\")\n",
        "\n",
        "    # Load and preprocess audio\n",
        "    y, sr = lb.load(audio_path)\n",
        "    yt, _ = lb.effects.trim(y)\n",
        "    mfcc = lb.feature.mfcc(y=yt, sr=sr, n_mfcc=20)\n",
        "    features = np.concatenate([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        [np.mean(lb.feature.spectral_rolloff(y=yt, sr=sr))],\n",
        "        [np.mean(lb.feature.rms(y=yt))]\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Predict\n",
        "    pred = model.predict(features)\n",
        "    pred_label = label_encoder.inverse_transform(pred)[0]\n",
        "    pred_user, pred_phrase = pred_label.split('_', 1)\n",
        "\n",
        "    # Verify authorized user (Eddy or Lievin) and valid sentence\n",
        "    if pred_user in ['Eddy', 'Lievin'] and pred_phrase in ['yes approve', 'confirm transaction']:\n",
        "        print(f\"Voice verification passed. Predicted: {pred_user} saying '{pred_phrase}'\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Access Denied: Predicted {pred_user} saying '{pred_phrase}', expected Eddy or Lievin saying 'yes approve' or 'confirm transaction'\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "InxVUx3Z19Fb"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_unauthorized_attempt(model, label_encoder):\n",
        "    print(\"\\n=== Unauthorized Voice Attempt Simulation ===\")\n",
        "\n",
        "    # Simulate random features (representing an unauthorized user)\n",
        "    features = np.concatenate([\n",
        "        np.random.randn(20) * 10,  # Random MFCC means\n",
        "        [np.random.uniform(1000, 5000)],  # Random rolloff\n",
        "        [np.random.uniform(0.01, 0.1)]  # Random energy\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Predict\n",
        "    pred = model.predict(features)\n",
        "    pred_label = label_encoder.inverse_transform(pred)[0]\n",
        "    pred_user, pred_phrase = pred_label.split('_', 1)\n",
        "    print(f\"Access Denied: Predicted {pred_user} saying '{pred_phrase}' (unauthorized user)\")"
      ],
      "metadata": {
        "id": "jTADVB9b98aF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load and combine features\n",
        "    X, labels, df_combined = load_and_combine_features('/content/audio_features(1).csv')\n",
        "\n",
        "    # Train model\n",
        "    model, label_encoder = train_rf_model(X, labels)\n",
        "\n",
        "    # Simulate authorized transaction (example: Eddy saying 'confirm transaction')\n",
        "    simulate_transaction(\n",
        "        '/content/confirm_eddy.mp3',\n",
        "        model=model,\n",
        "        label_encoder=label_encoder\n",
        "    )\n",
        "\n",
        "    # Simulate unauthorized attempt\n",
        "    simulate_unauthorized_attempt(model, label_encoder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdbgMK0O-PK0",
        "outputId": "cce2546b-f5d9-4c0f-e42c-94ec3170c7c7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved waveform for Eddy - confirm transaction: Duration 2.67s, clear speech peaks\n",
            "Saved spectrogram for Eddy - confirm transaction: Speech frequencies 0-5kHz\n",
            "Saved waveform for Lievin - confirm transaction: Duration 2.04s, clear speech peaks\n",
            "Saved spectrogram for Lievin - confirm transaction: Speech frequencies 0-5kHz\n",
            "Saved waveform for Eddy - yes approve: Duration 2.88s, clear speech peaks\n",
            "Saved spectrogram for Eddy - yes approve: Speech frequencies 0-5kHz\n",
            "Saved waveform for Lievin - yes approve: Duration 2.85s, clear speech peaks\n",
            "Saved spectrogram for Lievin - yes approve: Speech frequencies 0-5kHz\n",
            "\n",
            "Voiceprint RF Model - Accuracy: 1.0000, F1-Score: 1.0000, Log Loss: 0.0000\n",
            "\n",
            "=== Voiceprint Verification Simulation ===\n",
            "Voice verification passed. Predicted: Eddy saying 'confirm transaction'\n",
            "\n",
            "=== Unauthorized Voice Attempt Simulation ===\n",
            "Access Denied: Predicted Eddy saying 'yes approve' (unauthorized user)\n"
          ]
        }
      ]
    }
  ]
}